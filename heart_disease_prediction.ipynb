{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "669025da",
   "metadata": {},
   "source": [
    "### Importing Health Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46d632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./heart_cleveland_upload.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea7b61",
   "metadata": {},
   "source": [
    "### Identifying Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumofnull = df.isnull().sum()\n",
    "sumofnull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc607220",
   "metadata": {},
   "source": [
    "### Examining Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15319af",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = df.dtypes\n",
    "datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2106ae",
   "metadata": {},
   "source": [
    "### Identifying Numerical and Categorical Features\n",
    "Need to classify the features within our health data into two categories: numerical and categorical. This classification is crucial for our data analysis and modeling efforts. Understanding the nature of these features is essential for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f5790",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'condition']\n",
    "cat_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "\n",
    "print(\"Numeric Features:\")\n",
    "print(numeric_features)\n",
    "\n",
    "print(\"\\nCategorical Features:\")\n",
    "print(cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e56848b",
   "metadata": {},
   "source": [
    "### Converting Features to Categorical Data Types\n",
    "we transform selected features into categorical data types. Specifically, we convert 'sex,' 'cp,' 'fbs,' 'restecg,' 'exang,' 'slope,' 'ca,' and 'thal' into categorical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = cat_features\n",
    "df[lst] = df[lst].astype(object)\n",
    "dtype = df.dtypes\n",
    "\n",
    "dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2288e10",
   "metadata": {},
   "source": [
    "### Exploring Feature Correlations\n",
    "By generating a heatmap using the 'sns.heatmap' function, we visualize the relationships between numerical features . This visualization is pivotal for understanding how these features interact and impact each other within the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e139ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "selected_columns = df[numeric_features]\n",
    "\n",
    "# Calculating correlation\n",
    "corr_data = selected_columns.corr()\n",
    "\n",
    "# Creating heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_data, annot=True, cmap='RdBu', linewidths=0.1)\n",
    "plt.title('Correlation Between Numeric Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b59fff",
   "metadata": {},
   "source": [
    "### Visualizing Health Conditions\n",
    "Visualize the distribution of health conditions within our dataset. By generating this plot, we can gain a clear overview of the prevalence of different health conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb6bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "condition_ax = sns.countplot(x=df[\"condition\"], palette='bwr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322ba8bc",
   "metadata": {},
   "source": [
    "### Analyzing Health Conditions by Gender\n",
    "we generate a countplot to analyze health conditions with respect to gender. By creating this plot, we gain insights into how different health conditions are distributed among males and females. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb1e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_ax = sns.countplot(x=df[\"sex\"], hue=df['condition'],  palette='bwr')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930db80f",
   "metadata": {},
   "source": [
    "### Examining Chest Pain Types and Health Conditions\n",
    "countplot to examine the relationship between different types of chest pain ('cp') and health conditions. By visualizing this data, we gain insights into how various chest pain types are associated with different health conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59142a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_ax = sns.countplot(x=df[\"cp\"], hue=df['condition'], palette='bwr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3c94e",
   "metadata": {},
   "source": [
    "### Investigating Fasting Blood Sugar Levels and Health Conditions\n",
    "countplot to investigate the relationship between fasting blood sugar levels ('fbs') and health conditions. By visualizing this data, we gain insights into how different fasting blood sugar levels are associated with various health conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b5bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbs_ax = sns.countplot(x=df[\"fbs\"], hue=df['condition'], palette='bwr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfece26",
   "metadata": {},
   "source": [
    "### Analyzing Resting Electrocardiographic Results and Health Conditions\n",
    "countplot to analyze the connection between resting electrocardiographic results ('restecg') and health conditions. By visualizing this data, we gain insights into how different resting electrocardiographic outcomes are linked to various health conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3f40cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "restecg_ax = sns.countplot(x=df[\"restecg\"], hue=df['condition'], palette='bwr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a99888",
   "metadata": {},
   "source": [
    "### Examining Exercise-Induced Angina and Health Conditions\n",
    "countplot to examine the relationship between exercise-induced angina ('exang') and health conditions. By visualizing this data, we gain insights into how the presence or absence of exercise-induced angina is associated with various health conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8046142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exang_ax = sns.countplot(x=df[\"exang\"], hue=df['condition'], palette='bwr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe959bc",
   "metadata": {},
   "source": [
    "### Investigating the Slope of the ST Segment and Health Conditions\n",
    "countplot to investigate the relationship between the slope of the ST segment ('slope') and health conditions. By visualizing this data, we gain insights into how different ST segment slopes are associated with various health conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a794ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_ax = sns.countplot(x=df[\"slope\"], hue=df['condition'], palette='bwr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa4066",
   "metadata": {},
   "source": [
    "### Analyzing the Number of Major Vessels Colored by Fluoroscopy and Health Conditions\n",
    "countplot to analyze the relationship between the number of major vessels colored by fluoroscopy ('ca') and health conditions. By visualizing this data, we gain insights into how the number of colored vessels is associated with various health conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ea375",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_ax = sns.countplot(x=df[\"ca\"], hue=df['condition'], palette='bwr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d8de1",
   "metadata": {},
   "source": [
    "### Examining Thalassemia and Health Conditions\n",
    "countplot to examine the relationship between thalassemia ('thal') and health conditions. By visualizing this data, we gain insights into how different thalassemia categories are associated with various health conditions. This analysis is essential for understanding the impact of thalassemia on health outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18e9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "thal_ax = sns.countplot(x=df[\"thal\"], hue=df['condition'], palette='bwr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68679504",
   "metadata": {},
   "source": [
    "### Visualizing Age Distribution\n",
    "create a histogram to visualize the distribution of age in our dataset. The histogram is constructed using the 'age' column from the dataset and is presented with 20 bins for better visualization. This plot helps us gain insights into the distribution of ages within the dataset and is a critical aspect of our project's analysis to understand the age demographics of the individuals in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_col = df['age']\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(age_col, bins=20, color='skyblue', alpha=0.7, ec='blue')\n",
    "\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Age Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e569ac31",
   "metadata": {},
   "source": [
    "### Visualizing Resting Blood Pressure Distribution\n",
    " create a histogram to visualize the distribution of resting blood pressure ('trestbps') in our dataset. The histogram is constructed using the 'trestbps' column from the dataset and is presented with 20 bins for better visualization. This plot helps us gain insights into the distribution of resting blood pressure levels within the dataset and is an important aspect of our project's analysis to understand the distribution of this health-related feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "trestbps_col = df['trestbps']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(trestbps_col, bins=20, color='lightcoral', alpha=0.7, ec='red')\n",
    "\n",
    "plt.xlabel('trestbps')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('trestbps Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9601bed",
   "metadata": {},
   "source": [
    "### Visualizing Cholesterol Distribution\n",
    "create a histogram to visualize the distribution of cholesterol levels ('chol') in our dataset. The histogram is constructed using the 'chol' column from the dataset and is presented with 20 bins for better visualization. This plot helps us gain insights into the distribution of cholesterol levels within the dataset, which is a critical aspect of our project's analysis to understand the distribution of this health-related feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9891fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chol_col = df['chol']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(chol_col, bins=20, color='lightgreen', alpha=0.7, ec='green')\n",
    "\n",
    "# Label x-axis and y-axis, and set title\n",
    "plt.xlabel('Cholesterol (chol)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Cholesterol Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c157c8ff",
   "metadata": {},
   "source": [
    "### Visualizing Maximum Heart Rate Distribution\n",
    "create a histogram to visualize the distribution of maximum heart rate ('thalach') in our dataset. The histogram is constructed using the 'thalach' column from the dataset and is presented with 20 bins for better visualization. This plot helps us gain insights into the distribution of maximum heart rate levels within the dataset, which is a critical aspect of our project's analysis to understand the distribution of this health-related feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a06f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "thalach_col = df['thalach']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(thalach_col, bins=20, color='cyan', alpha=0.7, ec=\"darkblue\")\n",
    "\n",
    "# Label x-axis and y-axis, and set title\n",
    "plt.xlabel('Maximum Heart Rate (thalach)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Maximum Heart Rate Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bc45f2",
   "metadata": {},
   "source": [
    "### Visualizing ST Depression Distribution\n",
    "create a histogram to visualize the distribution of ST depression ('oldpeak') in our dataset. The histogram is constructed using the 'oldpeak' column from the dataset and is presented with 20 bins for better visualization. This plot helps us gain insights into the distribution of ST depression levels within the dataset, which is a critical aspect of our project's analysis to understand the distribution of this health-related feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad75cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "oldpeak_col = df['oldpeak']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(oldpeak_col, bins=20, color='orange', alpha=0.7, ec=\"darkred\")\n",
    "\n",
    "# Label x-axis and y-axis, and set title\n",
    "plt.xlabel('ST depression')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of ST depression')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6049aac",
   "metadata": {},
   "source": [
    "### Analyzing Fasting Blood Sugar Levels and Health Conditions\n",
    "use a countplot to analyze the relationship between fasting blood sugar levels ('fbs') and health conditions. By generating this plot, we gain insights into how different fasting blood sugar levels are associated with various health conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ccdc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "countplt = sns.catplot(x='fbs', hue='condition', kind='count', alpha=0.85, data=df, palette='bwr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8485fb9b",
   "metadata": {},
   "source": [
    "### Visualizing Chest Pain Types, Age, and Health Conditions\n",
    "use a violin plot to visualize the relationship between different chest pain types ('cp'), age, and health conditions. By generating this plot, we gain insights into how chest pain types are distributed across different age groups and their association with health conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41061a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "violinplt = sns.catplot(x='cp', y='age', hue='condition', kind='violin', palette='winter', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe0f552",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features\n",
    "We'll encode categorical features within our dataset, specifically 'cp,' 'thal,' and 'slope,' using one-hot encoding. We'll transform these categorical variables into a numerical format, to incorporate them into our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bb9993",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['cp', 'thal', 'slope']\n",
    "df['oldpeak'] = df['oldpeak'].astype(int)\n",
    "# Cast categorical columns to integer data type\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, prefix_sep='_', dtype=int)\n",
    "df_encoded.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23911ae3",
   "metadata": {},
   "source": [
    "### Preparing Features and Target Variable\n",
    "Prepare the features and the target variable for our analysis. We'll create the variable 'x' by excluding the 'condition' column, which serves as our feature set. The 'y' variable is defined as the 'condition' column, representing our target variable. This separation is fundamental for our project as it sets the stage for further data analysis, modeling, and understanding the relationship between the features and the health condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e0db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for predictor variables (features)\n",
    "x = df_encoded.drop('condition', axis=1)\n",
    "\n",
    "# Set 'y' as the target variable\n",
    "y = df_encoded['condition']\n",
    "\n",
    "\n",
    "print(\"Features (x):\")\n",
    "print(x.head())\n",
    "\n",
    "print(\"\\nTarget Variable (y):\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d3c7b0",
   "metadata": {},
   "source": [
    "### Scaling Features\n",
    "use the MinMaxScaler from the sklearn library to scale the feature set 'x.' Scaling is crucial for ensuring that all the features are on a similar scale, preventing any feature from dominating the analysis due to its magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a55f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the 'x' DataFrame\n",
    "x = scaler.fit_transform(x)\n",
    "\n",
    "#--- Inspect data ---\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4226d0cd",
   "metadata": {},
   "source": [
    "### Splitting the Data into Training and Testing Sets\n",
    "We'll split our dataset into training and testing sets using the train_test_split function from the sklearn library. The training set, 'X_train' and 'Y_train,' is designed to train our predictive models, while the testing set, 'X_test' and 'Y_test,' is reserved for evaluating the model's performance. By performing this data split, we ensure that our models are trained and tested on different data subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc78a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=0.8, test_size=0.2, random_state=4)\n",
    "\n",
    "\n",
    "print(\"Training set - Predictor variables (X_train):\", X_train.shape)\n",
    "print(\"Testing set - Predictor variables (X_test):\", X_test.shape)\n",
    "print(\"Training set - Target variable (Y_train):\", Y_train.shape)\n",
    "print(\"Testing set - Target variable (Y_test):\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8bcf5b",
   "metadata": {},
   "source": [
    "### Building and Evaluating Logistic Regression Model\n",
    "build a logistic regression model for our project. We use the sklearn library to create the 'lr_model' and train it using the training data, 'X_train' and 'Y_train.' Additionally, we assess the model's performance through cross-validation, with 10 folds, to estimate its accuracy. The 'lr_mean_score' represents the mean accuracy across the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730dc22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Create logistic regression model\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "lr_model.fit(X_train, Y_train)\n",
    "\n",
    "# Perform cross-validation with 10 folds\n",
    "lr_cv_results = cross_val_score(lr_model, X_train, Y_train, cv=10)\n",
    "\n",
    "# Calculate mean score from cross-validation\n",
    "lr_mean_score = round(lr_cv_results.mean(), 4)\n",
    "\n",
    "print(\"Mean accuracy from cross-validation:\", lr_mean_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46594d21",
   "metadata": {},
   "source": [
    "### Building and Evaluating Linear Discriminant Analysis Model\n",
    "construct a Linear Discriminant Analysis (LDA) model for our project using the sklearn library. We create the 'ldr_model' and train it with the training data, 'X_train' and 'Y_train.' Subsequently, we assess the model's performance through cross-validation with 10 folds, calculating the mean accuracy. The 'ldr_mean_score' represents this mean accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b665e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Create LDA model\n",
    "ldr_model = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Fit the model to the training data\n",
    "ldr_model.fit(X_train, Y_train)\n",
    "\n",
    "# Perform cross-validation with 10 folds\n",
    "ldr_cv_results = cross_val_score(ldr_model, X_train, Y_train, cv=10)\n",
    "\n",
    "# Calculate mean score from cross-validation\n",
    "ldr_mean_score = round(ldr_cv_results.mean(), 4)\n",
    "\n",
    "print(\"Mean accuracy from cross-validation:\", ldr_mean_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56d8bd3",
   "metadata": {},
   "source": [
    "### Building and Evaluating K-Nearest Neighbors (KNN) Model\n",
    "construct a K-Nearest Neighbors (KNN) model for our project using the sklearn library. We create the 'knn_model' and train it with the training data, 'X_train' and 'Y_train.' Subsequently, we assess the model's performance through cross-validation with 10 folds, calculating the mean accuracy. The 'knn_mean_score' represents this mean accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd554cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create KNN model\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "knn_model.fit(X_train, Y_train)\n",
    "\n",
    "# Perform cross-validation with 10 folds\n",
    "knn_cv_results = cross_val_score(knn_model, X_train, Y_train, cv=10)\n",
    "\n",
    "# Calculate mean score and standard deviation from cross-validation\n",
    "knn_mean_score = round(knn_cv_results.mean(), 4)\n",
    "knn_std_score = round(knn_cv_results.std(), 4)\n",
    "\n",
    "print(\"Mean accuracy from cross-validation:\", knn_mean_score)\n",
    "print(\"Standard deviation of scores:\", knn_std_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76146765",
   "metadata": {},
   "source": [
    "### Building and Evaluating Decision Tree Classifier Model\n",
    "build a Decision Tree Classifier model for our project using the sklearn library. We create the 'dt_model' and train it with the training data, 'X_train' and 'Y_train.' Subsequently, we assess the model's performance through cross-validation with 10 folds, calculating the mean accuracy. The 'dt_mean_score' represents this mean accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc2f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create Decision Tree Classifier model\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "dt_model.fit(X_train, Y_train)\n",
    "\n",
    "# Perform cross-validation with 10 folds\n",
    "dt_cv_results = cross_val_score(dt_model, X_train, Y_train, cv=10)\n",
    "\n",
    "# Calculate mean score from cross-validation\n",
    "dt_mean_score = round(dt_cv_results.mean(), 4)\n",
    "\n",
    "print(\"Mean accuracy from cross-validation:\", dt_mean_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f4f2a3",
   "metadata": {},
   "source": [
    "### Building and Evaluating Gaussian Naive Bayes Model\n",
    "construct a Gaussian Naive Bayes model for our project using the sklearn library. We create the 'gnb_model' and train it with the training data, 'X_train' and 'Y_train.' Subsequently, we assess the model's performance through cross-validation with 10 folds, calculating the mean accuracy. The 'gnb_mean_score' represents this mean accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4b675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create Gaussian Naive Bayes model\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "# Fit the model to the training data\n",
    "gnb_model.fit(X_train, Y_train)\n",
    "\n",
    "# Perform cross-validation with 10 folds\n",
    "gnb_cv_results = cross_val_score(gnb_model, X_train, Y_train, cv=10)\n",
    "\n",
    "# Calculate mean score from cross-validation\n",
    "gnb_mean_score = round(gnb_cv_results.mean(), 4)\n",
    "\n",
    "print(\"Mean accuracy from cross-validation:\", gnb_mean_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee8129",
   "metadata": {},
   "source": [
    "### Building and Evaluating Random Forest Classifier Model\n",
    "construct a Random Forest Classifier model for our project using the sklearn library. We create the 'rf_model' with 100 trees and a maximum of 3 features per split. The model is trained using the training data, 'X_train' and 'Y_train.' Subsequently, we assess the model's performance through cross-validation with 10 folds, calculating the mean accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea62f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the number of trees and maximum number of features\n",
    "num_trees = 100\n",
    "max_features = 'sqrt'\n",
    "\n",
    "# Create Random Forest Classifier model\n",
    "rf_model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf_model.fit(X_train, Y_train)\n",
    "\n",
    "# Perform cross-validation with 10 folds\n",
    "rf_cv_results = cross_val_score(rf_model, X_train, Y_train, cv=10)\n",
    "\n",
    "# Calculate mean score from cross-validation\n",
    "rf_mean_score = round(rf_cv_results.mean(), 4)\n",
    "\n",
    "print(\"Mean accuracy from cross-validation:\", rf_mean_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3185194d",
   "metadata": {},
   "source": [
    "### Building and Evaluating Support Vector Classifier (SVC) Model\n",
    "construct a Support Vector Classifier (SVC) model for our project using the sklearn library. We create the 'sv_model' and train it with the training data, 'X_train' and 'Y_train.' Subsequently, we assess the model's performance through cross-validation with 10 folds, calculating the mean accuracy. The 'sv_mean_score' represents this mean accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create Support Vector Classifier (SVC) model\n",
    "sv_model = SVC()\n",
    "\n",
    "# Fit the model to the training data\n",
    "sv_model.fit(X_train, Y_train)\n",
    "\n",
    "# Perform cross-validation with 10 folds\n",
    "sv_cv_results = cross_val_score(sv_model, X_train, Y_train, cv=10)\n",
    "\n",
    "# Calculate mean score from cross-validation\n",
    "sv_mean_score = round(sv_cv_results.mean(), 4)\n",
    "\n",
    "\n",
    "print(\"Mean accuracy from cross-validation:\", sv_mean_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1569ee28",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b8f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Use the selected model to predict on the test data\n",
    "y_pred = lr_model.predict(X_test)  \n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "\n",
    "# Create classification report\n",
    "cr = classification_report(Y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(cr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc6319",
   "metadata": {},
   "source": [
    "### Making Predictions with Gaussian Naive Bayes Model\n",
    "utilize the trained Gaussian Naive Bayes model to make predictions on new data. We provide a set of features in the 'data' variable and use the 'gnb_model' to predict the corresponding health condition outcome. This prediction helps us understand how the model classifies a new instance based on the provided features, which is a crucial aspect of our project's analysis and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e97ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0.254, 1, 0.487, 0.362,  ## age_scaled, sex, trestbps_scaled, chol\n",
    "             1, 0.5, 0.641, 1,  ## fbs, restecg_scaled, thalach_scaled, exang\n",
    "             0.672, 0.863, 0, 0,  ## oldpeak_scaled, ca_scaled, cp_0, cp_1\n",
    "             0, 1, 0, 0,  ## cp_2, cp_3, thal_0, thal_1\n",
    "             0, 1, 0, 1]]  ## thal_2, thal_3, slope_0, slope_1, slope_2\n",
    "\n",
    "# predict the result by passing the sample data available here to your model to make a prediction.\n",
    "prediction = lr_model.predict(data)\n",
    "\n",
    "\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa13d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1723dca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
